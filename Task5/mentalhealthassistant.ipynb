{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cu54YEf6gDJZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OjcdBcNgNoP",
        "outputId": "85ba8b38-7777-44f6-9845-049e287617c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 76673\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 12030\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 10943\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"empathetic_dialogues\"\n",
        ")\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWJ61k5zhY-T",
        "outputId": "8584fa5a-468c-408e-ad8a-5ecd8fbcfe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-677815203.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=38338, training_loss=3.011539232961585, metrics={'train_runtime': 4275.8437, 'train_samples_per_second': 35.863, 'train_steps_per_second': 8.966, 'total_flos': 5008601439535104.0, 'train_loss': 3.011539232961585, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./empathetic_chatbot\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "     report_to=[]  # Disable all reporting integrations including wandb\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g-q60mU0hvs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f763661d-7b87-481b-a047-056c49df6a2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('empathetic_distilgpt2/tokenizer_config.json',\n",
              " 'empathetic_distilgpt2/special_tokens_map.json',\n",
              " 'empathetic_distilgpt2/vocab.json',\n",
              " 'empathetic_distilgpt2/merges.txt',\n",
              " 'empathetic_distilgpt2/added_tokens.json',\n",
              " 'empathetic_distilgpt2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.save_pretrained(\"empathetic_distilgpt2\")\n",
        "tokenizer.save_pretrained(\"empathetic_distilgpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U-bsyNVRh9ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb1a41f-68ea-444a-92f7-4b1429c70d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empathetic Mental Health Chatbot ðŸ¤—\n",
            "Type 'exit' to end the conversation.\n",
            "\n",
            "You: i have done great work\n",
            "Bot: on my job and i am so happy for you!  I am so happy for you!\n",
            "You: exit\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def chat():\n",
        "    print(\"Empathetic Mental Health Chatbot ðŸ¤—\")\n",
        "    print(\"Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Move inputs to the same device as the model\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_length=100,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        print(\"Bot:\", response.replace(user_input, \"\").strip())\n",
        "\n",
        "\n",
        "# Run the chatbot\n",
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}