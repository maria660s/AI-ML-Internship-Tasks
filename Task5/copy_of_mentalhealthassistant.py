# -*- coding: utf-8 -*-
"""Copy of mentalhealthassistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-yNKx4szaC5OWpEUdijDt8NU9P9mTww
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import torch

from datasets import load_dataset

dataset = load_dataset(
    "empathetic_dialogues"
)

print(dataset)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

def preprocess(example):
    return tokenizer(
        example["utterance"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )

tokenized_dataset = dataset.map(preprocess, batched=True)

from transformers import AutoModelForCausalLM
from transformers import TrainingArguments
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer


model = AutoModelForCausalLM.from_pretrained("distilgpt2")

training_args = TrainingArguments(
    output_dir="./empathetic_chatbot",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    eval_strategy="epoch",
    num_train_epochs=2,
    logging_dir="./logs",
    save_strategy="epoch",
    load_best_model_at_end=True,
     report_to=[]  # Disable all reporting integrations including wandb
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

model.save_pretrained("empathetic_distilgpt2")
tokenizer.save_pretrained("empathetic_distilgpt2")

import torch

def chat():
    print("Empathetic Mental Health Chatbot ðŸ¤—")
    print("Type 'exit' to end the conversation.\n")

    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break

        # Tokenize input
        inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)

        # Move inputs to the same device as the model
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        # Generate response
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_length=100,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(output[0], skip_special_tokens=True)
        print("Bot:", response.replace(user_input, "").strip())


# Run the chatbot
chat()